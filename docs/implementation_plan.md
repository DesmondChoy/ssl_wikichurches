# SSL WikiChurches Implementation Plan

## Progress Status

| Phase | Description | Status |
|-------|-------------|--------|
| Phase 1 | Core Infrastructure | âœ… Complete |
| Phase 2 | Data Pipeline | âœ… Complete |
| Phase 3 | Metrics & Evaluation | âœ… Complete |
| Phase 4 | Visualization & Analysis | âœ… Complete |
| Phase 5 | Fine-Tuning Analysis | ðŸ”„ In Progress |
| Phase 6 | Interactive Analysis Tool | âœ… Complete |

**Last Updated:** 2026-02-01 (Phase 5 items 3-4 pending team discussion)

---

## Overview

Build a system to compare SSL model attention patterns against 631 expert-annotated bounding boxes on 139 WikiChurches images, measuring whether models attend to the same features human experts consider diagnostic.

**Models:** DINOv2, DINOv3, MAE, CLIP, SigLIP 2 (all ViT-B, evaluated frozen and fine-tuned)
**Research Design:** Two-pass analysis comparing attention patterns before and after task-specific fine-tuning
**Primary Metric:** IoU between thresholded attention and expert bounding boxes
**Platform:** M4 Pro with MPS backend

---

## Project Structure

```
ssl_wikichurches/
â”œâ”€â”€ app/                            # Interactive analysis tool
â”‚   â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ routers/                # API route handlers
â”‚   â”‚   â”œâ”€â”€ services/               # Business logic
â”‚   â”‚   â””â”€â”€ main.py                 # FastAPI entry point
â”‚   â”œâ”€â”€ frontend/                   # React + TypeScript frontend
â”‚   â””â”€â”€ precompute/                 # Pre-computation scripts
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter notebooks
â”‚   â””â”€â”€ 01_data_exploration.ipynb   # Dataset exploration with Polars
â”‚
â”œâ”€â”€ src/ssl_attention/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py              # ExperimentConfig dataclass
â”‚   â”‚   â””â”€â”€ models.py            # Model-specific configs
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ protocols.py         # VisionBackbone Protocol, ModelOutput
â”‚   â”‚   â”œâ”€â”€ registry.py          # Model registry with lazy loading
â”‚   â”‚   â”œâ”€â”€ base.py              # BaseVisionModel ABC
â”‚   â”‚   â”œâ”€â”€ dinov2.py            # DINOv2 wrapper (facebook/dinov2-with-registers-base)
â”‚   â”‚   â”œâ”€â”€ dinov3.py            # DINOv3 wrapper (facebook/dinov3-vitb16-pretrain-lvd1689m)
â”‚   â”‚   â”œâ”€â”€ mae.py               # MAE wrapper (facebook/vit-mae-base)
â”‚   â”‚   â”œâ”€â”€ clip_model.py        # CLIP wrapper (openai/clip-vit-base-patch16)
â”‚   â”‚   â””â”€â”€ siglip.py            # SigLIP 2 wrapper (google/siglip2-base-patch16-224)
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cls_attention.py     # CLS token attention extraction
â”‚   â”‚   â”œâ”€â”€ rollout.py           # Attention rollout implementation
â”‚   â”‚   â”œâ”€â”€ gradcam.py           # GradCAM for transformers
â”‚   â”‚   â””â”€â”€ utils.py             # Upsampling, normalization
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ wikichurches.py      # WikiChurchesDataset, AnnotatedSubset
â”‚   â”‚   â”œâ”€â”€ transforms.py        # Model-specific preprocessing
â”‚   â”‚   â””â”€â”€ annotations.py       # BoundingBox, ImageAnnotation dataclasses
â”‚   â”‚
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ iou.py               # IoU computation (attention vs bbox)
â”‚   â”‚   â”œâ”€â”€ baselines.py         # Random, center, saliency baselines
â”‚   â”‚   â””â”€â”€ statistics.py        # t-tests, bootstrap CIs, effect sizes
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ linear_probe.py      # Linear classifier training
â”‚   â”‚   â”œâ”€â”€ fine_tuning.py       # Full backbone fine-tuning
â”‚   â”‚   â””â”€â”€ ablations.py         # Layer analysis, model comparison
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ heatmaps.py          # Attention heatmap generation
â”‚   â”‚   â”œâ”€â”€ overlays.py          # Bbox + attention overlay
â”‚   â”‚   â””â”€â”€ plots.py             # Statistical plots
â”‚   â”‚
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ manager.py           # HDF5 feature caching
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ device.py            # MPS/CUDA/CPU handling
â”‚       â””â”€â”€ logging.py           # Structured logging
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ default.yaml         # Main experiment config
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ extract_features.py
â”‚       â”œâ”€â”€ compute_attention.py
â”‚       â”œâ”€â”€ run_iou_analysis.py
â”‚       â”œâ”€â”€ train_linear_probe.py
â”‚       â””â”€â”€ fine_tune_models.py  # Fine-tuning script
â”‚
â”œâ”€â”€ outputs/                     # Git-ignored
â”‚   â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ figures/
â”‚
â””â”€â”€ tests/
```

---

## Model Details (Latest APIs)

| Model | HuggingFace ID | Class | Patch Size | Notes |
|-------|----------------|-------|------------|-------|
| DINOv2 | `facebook/dinov2-with-registers-base` | `AutoModel` | 14 | Registers give cleaner attention maps |
| DINOv3 | `facebook/dinov3-vitb16-pretrain-lvd1689m` | `DINOv3ViTModel` | 16 | Requires transformers 4.56.0+, uses RoPE |
| MAE | `facebook/vit-mae-base` | `ViTMAEModel` | 16 | Set `mask_ratio=0.0` for extraction |
| CLIP | `openai/clip-vit-base-patch16` | `CLIPVisionModel` | 16 | Vision encoder only |
| SigLIP 2 | `google/siglip2-base-patch16-224` | `Siglip2VisionModel` | 16 | Improved dense features; better for attention alignment |

### Key API Patterns

```python
# All models support:
outputs = model(**inputs, output_attentions=True)
attentions = outputs.attentions  # Tuple of (batch, heads, seq, seq) per layer

# DINOv2/v3 with registers:
# Sequence = [CLS] + [REG1-4] + [patches]
num_registers = model.config.num_register_tokens  # 4
cls_to_patches = attn[:, :, 0, 1 + num_registers:]

# MAE (disable masking):
config = ViTMAEConfig.from_pretrained(model_id, mask_ratio=0.0)
model = ViTMAEModel.from_pretrained(model_id, config=config)
```

---

## Implementation Phases

### Phase 1: Core Infrastructure âœ… COMPLETE

1. **Update `pyproject.toml`** âœ…
   ```toml
   dependencies = [
       "torch>=2.1.0",
       "torchvision>=0.16.0",
       "transformers>=4.56.0",  # For DINOv3 support
       "timm>=1.0.20",
       "pillow>=10.0.0",
       "h5py>=3.10.0",
       "numpy>=1.26.0",
       "scipy>=1.11.0",
       "matplotlib>=3.8.0",
       "seaborn>=0.13.0",
       "scikit-learn>=1.3.0",
       "pyyaml>=6.0.1",
   ]
   ```

2. **Create protocols** (`src/ssl_attention/models/protocols.py`) âœ…
   ```python
   @dataclass
   class ModelOutput:
       cls_token: Tensor           # (B, D)
       patch_tokens: Tensor        # (B, N, D)
       attention_weights: list[Tensor]  # List of (B, H, N+1, N+1)

   @runtime_checkable
   class VisionBackbone(Protocol):
       patch_size: int
       embed_dim: int
       num_layers: int
       def forward(self, images: Tensor) -> ModelOutput: ...
   ```

3. **Implement model wrappers** (separate files): âœ…
   - `dinov2.py` - Handle registers, patch size 14 âœ…
   - `dinov3.py` - Handle RoPE, registers, patch size 16 âœ…
   - `mae.py` - Disable masking with `mask_ratio=0.0` âœ…
   - `clip_model.py` - Vision encoder only âœ…
   - `siglip.py` - Vision encoder only (SigLIP 2) âœ…

4. **Implement attention extractors**: âœ…
   - `cls_attention.py` - CLS to patch attention with head fusion âœ…
   - `rollout.py` - Attention rollout through layers âœ…
   - `gradcam.py` - Gradient-based baseline âœ…

### Phase 2: Data Pipeline âœ… COMPLETE

1. **Annotation parsing** (`annotations.py`) âœ…
   - `BoundingBox` with `to_mask(H, W)` method
   - `ImageAnnotation` parsing `building_parts.json`
   - Handle normalized (0-1) coordinates with clamping

2. **Dataset classes** (`wikichurches.py`) âœ…
   - `AnnotatedSubset` - 139 images with bboxes
   - `FullDataset` - 9,485 images for linear probe
   - Per-model preprocessing via registry

3. **HDF5 caching** (`cache/manager.py`) âœ…
   - Cache features and attention maps
   - Key: `{model}/{layer}/{image_id}`
   - LRU eviction and corruption detection

### Phase 3: Metrics & Evaluation âœ… COMPLETE

1. **IoU computation** (`metrics/iou.py`) âœ…
   - Threshold at percentiles (90/80/70/60/50)
   - IoU against bbox union with coverage metric
   - Per-bbox breakdown
   - CorLoc@50 for literature comparison

2. **Pointing game** (`metrics/pointing_game.py`) âœ…
   - Binary hit detection (max attention in bbox)
   - Top-k pointing accuracy
   - Per-feature-type breakdown

3. **Baselines** (`metrics/baselines.py`) âœ…
   - Random uniform
   - Center Gaussian
   - Sobel edge saliency
   - Saliency prior (center + border suppression)

4. **Statistics** (`metrics/statistics.py`) âœ…
   - Paired t-test / Wilcoxon signed-rank
   - Bootstrap CIs (10k samples)
   - Cohen's d effect size
   - Holm multiple comparison correction

5. **Linear probe** (`evaluation/linear_probe.py`) âœ…
   - Train on frozen CLS features (sklearn LogisticRegression)
   - Stratified k-fold cross-validation
   - Accuracy, F1, per-class accuracy, confusion matrix

### Phase 4: Visualization & Analysis âœ… COMPLETE

1. **Heatmaps** (`visualization/heatmaps.py`) âœ…
   - Upsample to original resolution
   - Colormap overlay with configurable colormaps

2. **Overlays** (`visualization/overlays.py`) âœ…
   - Bounding box drawing with labels
   - Attention heatmap overlay on images

3. **Comparison plots** (`visualization/plots.py`) âœ…
   - Model comparison bar charts with CIs
   - Layer-wise progression
   - Per-feature-category breakdown
   - Style breakdown charts
   - Scatter plots for coverage vs IoU

### Phase 5: Fine-Tuning Analysis ðŸ”„ IN PROGRESS

1. **Fine-tuning implementation** (`evaluation/fine_tuning.py`) âœ…
   - `FineTuningConfig` dataclass for hyperparameters
   - `FineTuningResult` dataclass for training metrics and checkpoint path
   - `FineTunableModel` wrapping SSL backbone + classification head
   - `FineTuner` class with training loop, stratified split, class weighting
   - `ClassificationHead` linear classifier on CLS token
   - Differential learning rates for backbone vs head
   - MPS memory management (`torch.mps.empty_cache()`), checkpoint saving
   - `load_finetuned_model()` for loading trained checkpoints
   - `save_training_results()` for JSON export of training history

2. **Fine-tuning script** (`experiments/scripts/fine_tune_models.py`) âœ…
   - Train single model or all models via CLI flags
   - Configurable hyperparameters (epochs, batch size, learning rates)
   - Head-only training option (`--freeze-backbone`)
   - Save checkpoints: `outputs/checkpoints/{model}_finetuned.pt`
   - Training summary with per-model results

3. **Comparative analysis** â¬œ
   - Load fine-tuned models
   - Extract attention on annotated subset
   - Compute Î” IoU per model
   - Statistical tests (paired t-test on per-image IoU)

4. **Visualization** â¬œ
   - Side-by-side heatmaps (frozen vs fine-tuned)
   - Attention shift maps (where did attention move?)

> **Note:** Items 3-4 are pending team discussion on fine-tuning strategy (what type of fine-tuning to pursue, evaluation approach, and next steps).

### Phase 6: Interactive Analysis Tool âœ… COMPLETE

**Technology Choice:** React + FastAPI (full control, production-ready)

1. **Backend** (`app/backend/`) âœ…
   - FastAPI with routers for images, attention, metrics, comparison
   - Services for image loading, metrics querying (SQLite), caching
   - Pre-computation scripts for attention maps, heatmaps, and metrics

2. **Frontend** (`app/frontend/`) âœ…
   - React + TypeScript + Vite
   - Image browser with style filtering
   - Attention viewer with model/layer selection
   - Model comparison views

3. **Pre-computation Pipeline** âœ…
   - `generate_attention_cache.py` - Extract attention to HDF5
   - `generate_heatmap_images.py` - Render heatmap overlays as PNGs
   - `generate_metrics_cache.py` - Compute IoU to SQLite

4. **API Endpoints** âœ…
   - `/api/images` - Image listing, filtering, serving
   - `/api/attention` - Heatmap and overlay serving
   - `/api/metrics` - IoU metrics, leaderboard, layer progression
   - `/api/compare` - Model comparison, frozen vs fine-tuned

---

## Critical Files to Create

| Priority | File | Purpose | Status |
|----------|------|---------|--------|
| 1 | `pyproject.toml` | Add ML dependencies | âœ… Done |
| 2 | `src/ssl_attention/models/protocols.py` | Core abstractions | âœ… Done |
| 3 | `src/ssl_attention/models/dinov2.py` | DINOv2 wrapper | âœ… Done |
| 4 | `src/ssl_attention/models/dinov3.py` | DINOv3 wrapper | âœ… Done |
| 5 | `src/ssl_attention/models/mae.py` | MAE wrapper | âœ… Done |
| 6 | `src/ssl_attention/models/clip_model.py` | CLIP wrapper | âœ… Done |
| 7 | `src/ssl_attention/models/siglip.py` | SigLIP wrapper | âœ… Done |
| 8 | `src/ssl_attention/attention/cls_attention.py` | Primary attention method | âœ… Done |
| 9 | `src/ssl_attention/data/annotations.py` | Bbox parsing | âœ… Done |
| 10 | `src/ssl_attention/data/wikichurches.py` | Dataset classes | âœ… Done |
| 11 | `src/ssl_attention/cache/manager.py` | HDF5 caching | âœ… Done |
| 12 | `src/ssl_attention/metrics/iou.py` | Primary metric | âœ… Done |
| 13 | `src/ssl_attention/metrics/pointing_game.py` | Pointing game metric | âœ… Done |
| 14 | `src/ssl_attention/metrics/baselines.py` | Baseline generators | âœ… Done |
| 15 | `src/ssl_attention/metrics/statistics.py` | Statistical tests | âœ… Done |
| 16 | `src/ssl_attention/evaluation/linear_probe.py` | Linear probe evaluation | âœ… Done |
| 17 | `src/ssl_attention/evaluation/fine_tuning.py` | Fine-tuning wrapper | âœ… Done |
| 18 | `experiments/scripts/fine_tune_models.py` | Training script | âœ… Done |
| 19 | `app/backend/main.py` | Interactive analysis tool backend | âœ… Done |
| 20 | `app/frontend/` | Interactive analysis tool frontend | âœ… Done |

### Additional Phase 1 Files Created

| File | Purpose | Status |
|------|---------|--------|
| `src/ssl_attention/models/base.py` | BaseVisionModel ABC | âœ… Done |
| `src/ssl_attention/models/registry.py` | Model registry with lazy loading | âœ… Done |
| `src/ssl_attention/attention/rollout.py` | Attention rollout implementation | âœ… Done |
| `src/ssl_attention/attention/gradcam.py` | GradCAM for transformers | âœ… Done |
| `src/ssl_attention/config.py` | Centralized configuration | âœ… Done |
| `src/ssl_attention/utils/device.py` | MPS/CUDA/CPU handling | âœ… Done |

### Additional Phase 2 Files Created

| File | Purpose | Status |
|------|---------|--------|
| `src/ssl_attention/data/__init__.py` | Data module exports | âœ… Done |
| `src/ssl_attention/data/transforms.py` | Model-specific preprocessing | âœ… Done |
| `src/ssl_attention/cache/__init__.py` | Cache module exports | âœ… Done |

### Additional Phase 3 Files Created

| File | Purpose | Status |
|------|---------|--------|
| `src/ssl_attention/metrics/__init__.py` | Metrics module exports | âœ… Done |
| `src/ssl_attention/evaluation/__init__.py` | Evaluation module exports | âœ… Done |

### Additional Phase 4 Files Created

| File | Purpose | Status |
|------|---------|--------|
| `src/ssl_attention/visualization/__init__.py` | Visualization module exports | âœ… Done |
| `src/ssl_attention/visualization/heatmaps.py` | Attention heatmap generation | âœ… Done |
| `src/ssl_attention/visualization/overlays.py` | Bbox + attention overlay | âœ… Done |
| `src/ssl_attention/visualization/plots.py` | Statistical plots | âœ… Done |
| `notebooks/01_data_exploration.ipynb` | Dataset exploration with Polars | âœ… Done |

### Phase 5 Files Created

| File | Purpose | Status |
|------|---------|--------|
| `src/ssl_attention/evaluation/fine_tuning.py` | Fine-tuning wrapper with FineTunableModel | âœ… Done |
| `experiments/scripts/fine_tune_models.py` | CLI training script | âœ… Done |

### Phase 6 Files Created

| File | Purpose | Status |
|------|---------|--------|
| `app/backend/main.py` | FastAPI application entry | âœ… Done |
| `app/backend/config.py` | Backend configuration | âœ… Done |
| `app/backend/schemas.py` | Pydantic schemas | âœ… Done |
| `app/backend/routers/` | API route handlers | âœ… Done |
| `app/backend/services/` | Business logic services | âœ… Done |
| `app/precompute/` | Pre-computation scripts | âœ… Done |
| `app/frontend/` | React + TypeScript frontend | âœ… Done |

---

## Verification Plan

1. **Unit tests**
   - Model outputs have correct shapes
   - Attention has expected dimensions per model
   - IoU computes correctly on synthetic data

2. **Smoke test**
   - Run full pipeline on 5 images
   - Verify cache writes/reads work
   - Check attention maps are sensible

3. **Baseline sanity check**
   - Random baseline IoU ~5-10%
   - Center baseline slightly higher
   - Model attention should beat both

4. **Linear probe**
   - >50% accuracy on 4-class task
   - Confirms features are discriminative

5. **Visual inspection**
   - Attention overlays on sample images
   - Compare across models qualitatively

6. **Fine-tuning verification**
   - Training converges (loss decreases, val accuracy improves)
   - Fine-tuned models load correctly
   - Attention extraction works on fine-tuned models
   - IoU comparison shows measurable difference

7. **Interactive tool verification**
   - All 139 images load correctly
   - Model/method/layer selectors function
   - Comparison view synchronizes properly
   - Metrics dashboard displays correct values
   - Responsive on target browsers

---

## Key Design Decisions

1. **Separate model files** - DINOv2 and DINOv3 have different APIs (registers, RoPE, sequence length)
2. **Protocol-based abstractions** - Unified interface despite API differences
3. **Lazy loading with LRU cache** - Max 1-2 models in memory
4. **HDF5 caching** - Avoid recomputing features
5. **Percentile thresholding** - More robust than fixed thresholds
6. **Registers-aware extraction** - Skip register tokens for DINOv2/v3
7. **Two-pass evaluation** - Compare frozen and fine-tuned attention to isolate effect of task-specific training

---

## Notes

- DINOv2 sequence: 1 CLS + 4 registers + 256 patches (patch 14, 224px)
- DINOv3 sequence: 1 CLS + 4 registers + 196 patches (patch 16, 224px)
- MAE sequence: 1 CLS + 196 patches (with mask_ratio=0)
- CLIP/SigLIP 2: 1 CLS + 196 patches

### Dataset Details (Verified)

- **Annotation file:** `building_parts.json` with nested structure:
  - `meta`: 106 feature type definitions with hierarchical parent relationships
  - `annotations`: 139 images with bounding box groups
- **Coordinate format:** `left, top, width, height` (normalized 0-1, some edge values slightly negativeâ€”clamp to [0,1])
- **Style IDs:** Wikidata Q-IDs requiring mapping:
  - `Q46261` â†’ Romanesque (54 churches, 39%)
  - `Q176483` â†’ Gothic (49 churches, 35%)
  - `Q236122` â†’ Renaissance (22 churches, 16%)
  - `Q840829` â†’ Baroque (17 churches, 12%)
- **Bbox structure:** `annotations[image_id].bbox_groups[].elements[]` (nested, grouped by related features)

### Technical Notes

- MPS may need `torch.mps.empty_cache()` between batches
